1. CPU: Barnes-hut is faster than naive

1k bodies on naive approach cpu ~ 12.000 ms
1k bodies on barnes-hut approach cpu ~ 9.000 ms

10k bodies on naive approach cpu ~ 1.200.000 ms (100x slower)
10k bodies on barnes-hut approach cpu ~ 240.000 ms (25x slower)

2. Cpu barnes-hut and gpu (initialization only) barnes-hut are similar

1k bodies both ~ 9.000 ms
10k bodies both ~ 240.000 ms (25x slower)

but cpu initialization is ~ 0/1 ms, while gpu is ~ 220ms, possibly because of the transfer

BUT: the copying back to cpu is only needed for this approach of both cpu and gpu in one file, that can be removed

3. CPU: Building the tree is very fast (1ms) compared to force calculation (68 ms last step and 31ms first step) (10.000 bodies and 100 steps)


Conclusions:

- Quadtree is being created in each simualtion step and will change its size. 
So since we cant implement the tree creation on gpu, we will create it on cpu (which we saw is not computationally expensive), 
and then transfer it to gpu. So in terms of allocating this memory on gpu, we have two options. 
1: In each simulation step, we create the tree and based on its size, we allocate that much memory on gpu and at the end of the iteration, 
we deallocate this memory. This is not a good approach becasue allocations and deallocations are slow. 
2: We have a predefined max size of the quadtree and we will allocate this much memory at the start and then use it in each iteration 
and deallocate at the very end. The quadtree for particles by default does not have a max size, it all depends on the positions 
of the particles. Particles that are extremly close will go very deep into the tree. So to include the max size, we need to limit the maximum 
depth of the tree. And then, some particles that are close to each other will end up in the same tree node (quadrant). 
Now, looking at our quadrant structure, we do have this particle_index which is storing information about which particle is in that 
quadrant. So now since we can have multiple particles in the same quadrant at max depth, do we need to change this strucure? 
One option is to include a new array in the quadrant to keep track of all the particles that are grouped together. 
However, looking at our force calcualtion implementation, the only time we are using this particle_index is to make sure we 
dont calculate the particle's force with itself (and we cover that case with negative particle index). So, what if we ge to the point where we look at the max depth quadrant and 
its one particle_index, even though the current particle index might also be inside that quadrant, we dont care, because this 
is the max depth quadrant and we will in any case concider it one group and calculate the force with the whole quadrant.


Max size of stack is changed to max_depth * 3, because in worst case scenario we will have 3 children pushed on stack per level while traversing. (I think)

The results are not completely pricise, e.g. with 1000 bodies, they start to diviate slightly around 45th iteration and then the changes get bigger and bigger. but the final trees are nearly identical

Stack in kernel code is possibly fitting in register memory which is the fastest.

Device Name: Tesla T4
Shared Memory per SM: 64 KB
Maximum Shared Memory per Block: 48 KB
Shared Memory Available for Dynamic Allocation: 64 K
Number of SMs: 40

Challenge: Quadtree can be potentionally large, depending on the max_depth, and cant fit into shared memory.
Possible solutions:
    - lower max_depth -> leads to less size and less acurracy
    - use float instead of double -> halves the size but lower acurracy
    - calculate quadrant bounds on the fly instead of keeping 4 values for each quadrant -> lowers size but increases computation

The tree is actually far from its theoretical maximum size - 4^max_depth, and in the initial iteration is roughly 3 * n_bodies 
and later on gets much smaller, so in pracrise, in the first iteration it will not fit into shared memory, but after it will.

We will concider 4*n_bodies to be the practical limit of the tree which is relevant in the 
beggining of the simulation, and the reason it is 4 is becasue 4 is the branching factor 
of quadtree and the initial posisitons are unifrmly distributed, making the tree balanced 
with each body being in a leaf

80 * 1024 threads, 10 simulation:
barnes-hut basic shared mem ~ 1700ms
naive global ~ 4000ms

40 * 1024 threads, 10 simulations
barnes-hut basic shared mem ~ 1700ms
naive global ~ 21000ms